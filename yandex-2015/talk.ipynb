{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:0528a81d3e52356df49e322b107056109e5f2eb7f3253e85a5ddbc8b08357433"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tensor train decompositions\n",
      "## Ivan Oseledets\n",
      "## Skolkovo Institute of Science and Technology (Skoltech), Russia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Link to this talk: http://bit.ly/1xsnRqg"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## What is a tensor? \n",
      "A tensor is a $d$-way array, $A(i_1, \\ldots, i_d)$. For $d=2$ we get matrices.  \n",
      "\n",
      "A **tensor decomposition is a (compact) representation of a tensor using fewer number of parameters.  \n",
      "\n",
      "\n",
      "Review: [T. Kolda, B. Bader, Tensor decompositions](http://epubs.siam.org/doi/abs/10.1137/07070111X)\n",
      "\n",
      "They appear in many applications:  \n",
      "\n",
      "multivariate functions approximations, quantum mechanics, multiparametric problems, hypergraphs, multidimensional data..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## General setup\n",
      "\n",
      "We select a certain **manifold** $\\mathcal{M}$ of tensors with small dimension, and try to solve:\n",
      "\n",
      "\n",
      "1. Optimization problems, $F(X) \\rightarrow \\min$ with $X \\in \\mathcal{M}$  \n",
      "\n",
      "2. (or) Using apriopri knowledge of $X \\in \\mathcal{M}$ recover $X$ **by sampling**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##  Multivariate functions \n",
      "Many applications involve computation of a complex function of several parameters:\n",
      "\n",
      "$$E = f(p_1, \\ldots, p_M)$$\n",
      "\n",
      "Any single function computation can be **time-consuming. ** \n",
      "\n",
      "A good interpolation scheme might be a good idea."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Multivariate function interpolation\n",
      "There are **not too many** efficient methods for working with multivariate functions:\n",
      "\n",
      "- Radial basis functions\n",
      "- Sparse grids\n",
      "- Best $N$-term approximation (wavelets?)\n",
      "- Different machine learning techniques (decision trees, neural networks)\n",
      "\n",
      "Many successful methods are based on the idea of **separation of variables**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Separation of variables\n",
      "Function is called **separable**, if \n",
      "$$ f(x_1, \\ldots, x_d) = f_1(x_1) f_2(x_2) \\ldots f_d(x_d).$$\n",
      "\n",
      "Not many functions can be represented in this form  \n",
      "\n",
      "(or even approximated, it means **independence** of variables in some sense.)\n",
      "\n",
      "A more general and efficient class of functions is the **sum of separable functions**:  \n",
      "\n",
      "$$f(x_1,\\ldots, x_d) \\approx \\sum_{\\alpha=1}^r f^{(\\alpha)}_1(x_1) \\ldots f^{(\\alpha)}_d(x_d)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Fitting by sums of separable functions\n",
      "How to fit a given function $f$ by a sums of separable functions?  \n",
      "\n",
      "We can always consider $x_1, \\ldots, x_d$ to be discrete, i.e\n",
      "\n",
      "$$A(i_1, \\ldots, i_d) = f(x_1(i_1), \\ldots, x_d(i_d))$$  \n",
      "\n",
      "Second, we approximate the tensor by **sampling**.  \n",
      "Why it is possible to be done?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Number of parameters\n",
      "In the discrete case, representation\n",
      "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha)$$\n",
      "is called **canonical format**, or canonical-polyadic (CP-format).  \n",
      "The  number of parameters is $dnr$, so if $r$ is small, a good idea is that it is possible to fit those parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Matrix case\n",
      "In two dimensions separation boils down to **low-rank approximation**:\n",
      "$$A \\approx UV^{\\top}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Skeleton decomposition\n",
      "How can we approximation a low-rank matrix without computing all its elements?  \n",
      "The skeleton decomposition gives an answer: if a matrix has rank $r$, it can be exactly recovered from its $r$ linearly independent columns and $r$ linearly independent rows.  \n",
      "<img src =\"cross-pic.png\" >"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Cross approximation\n",
      "Cross approximation is a heuristic sampling technique for the selection of \"good\" rows and columns in a low-rank matrix.  \n",
      "\n",
      "The goal is to find rows and column that **maximize the volume** of the submatrix.  \n",
      "\n",
      "The maximal volume principle (Goreinov, Tyrtyshnikov) states that\n",
      "\n",
      "$$  \\Vert A - A_{skel} \\Vert_C \\leq (r + 1) \\sigma_{r+1},$$\n",
      "\n",
      "and $\\Vert \\cdot \\Vert$ is the maximal in modulus element of a matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We can do a short demo (and compare with the SVD)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from rect_cross2d import rect_cross2d \n",
      "from scipy.linalg import hilbert \n",
      "a = hilbert(1000)\n",
      "%timeit U, S, V = rect_cross2d(a, 1e-5)\n",
      "%timeit U, S, V = np.linalg.svd(a)\n",
      "U, S, V = rect_cross2d(a, 1e-5)\n",
      "print 'True error:',  np.linalg.norm(U.dot(np.diag(S).dot(V)) - a)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 28 ms per loop\n",
        "1 loops, best of 3: 742 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "True error:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.08067407133e-05\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Multivariate case: problems\n",
      "There is a big problem in many dimensions:  \n",
      "\n",
      "it is not possible to do the exact interpolation trick for the **sum-of-products** (canonical format).  \n",
      "\n",
      "The problem is not with the algorithms but with the **format** itself:  \n",
      "\n",
      "the best approximation may not even exist!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Bad example\n",
      "$$f(x_1, \\ldots, x_d) = x_1 + \\ldots + x_d$$.\n",
      "\n",
      "$$g(t) = (1 + x_1 t) (1 + x_2 t) \\ldots (1 + x_d t)$$\n",
      "\n",
      "$$g'(0) = f = \\underbrace{\\frac{g(h) - g(0)}{h}}_{\\mathrm{rank~2}} + \\mathcal{O}(h),$$\n",
      "\n",
      "but no **exact rank 2** exist!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## New tensor formats\n",
      "It motivated the development of new tensor formats, that are stable as the singular value decomposition.  \n",
      "\n",
      "The **Tensor Train (TT)** format is the one that I use the most: it is simple and efficient  \n",
      "\n",
      "$$A(i_1, \\ldots, i_d) \\approx G_1(i_1) \\ldots G_d(i_d),$$\n",
      "where $G_k(i_k)$ has  size $r_{k-1} \\times r_k$, $r_0 = r_d = 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## TT-format in a nutshell\n",
      "\n",
      "- If there is a canonical representation with rank $R$, then $r_k \\leq R$\n",
      "- TT-ranks are computable via SVD of auxiliary matrices\n",
      "- Basic linear algebra operations\n",
      "- Solve optimization problems with solution in the TT-format\n",
      "- **There is an exact interpolation formula** with $\\mathcal{O}(dnr^2)$ parameters (TT-cross method)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Packages\n",
      "We have two basic packages:\n",
      "- TT-Toolbox (http://github.com/oseledets/TT-Toolbox) which is a MATLAB Toolbox that implements all the basic algorithms \n",
      "- ttpy (http://github.com/oseledets/ttpy) - its Python counterpart\n",
      "\n",
      "Many basic and advanced algorithms are implemented, and can be used as building blocks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Demo on the cross approximation of tensors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tt\n",
      "from tt.cross import rect_cross\n",
      "\n",
      "d = 20\n",
      "n = 10\n",
      "x0 = tt.rand(d, n, 3)\n",
      "h = 1e-2\n",
      "def myfun1(x):\n",
      "    return np.sum(x, axis=1)\n",
      "    #return np.sqrt((((h * x) ** 2).sum(axis=1)))\n",
      "\n",
      "x1 = rect_cross(myfun1, x0)\n",
      "x1 = x1.round(1e-8)\n",
      "print x1"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "swp: 0/9 er_rel = 2.1e+00 er_abs = 6.4e+08 erank = 7.0 fun_eval: 8400\n",
        "swp: 1/9 er_rel = 7.6e-16 er_abs = 2.3e-07 erank = 11.7 fun_eval: 36120\n",
        "This is a 10-dimensional tensor \n",
        "r(0)=1, n(0)=20 \n",
        "r(1)=2, n(1)=20 \n",
        "r(2)=2, n(2)=20 \n",
        "r(3)=2, n(3)=20 \n",
        "r(4)=2, n(4)=20 \n",
        "r(5)=2, n(5)=20 \n",
        "r(6)=2, n(6)=20 \n",
        "r(7)=2, n(7)=20 \n",
        "r(8)=2, n(8)=20 \n",
        "r(9)=2, n(9)=20 \n",
        "r(10)=1 \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Advanced methods\n",
      "General strategy: formulate the problem as  a minimization problem\n",
      "\n",
      "$$ F(X) \\rightarrow \\min, \\quad X \\in \\mathcal{M}. $$\n",
      "\n",
      "- Linear systems: $F(x) = (Ax, x) - 2(f, x)$, where $x$ can be mapped to a tensor\n",
      "- Eigenvalue problems: $F(x) = (Ax, x)/(x, x)$\n",
      "- Dynamic problem: $f(x) = \\Vert \\frac{dx}{dt} - \\frac{dA}{dt} \\Vert$, "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Minimization techniques\n",
      "- Idea 1: Use ordinary optimization method + projection: $X_{k+1} = P(X_k + G(X_k))$\n",
      "- Idea 2: Use alternating least squares (polylinear structure)\n",
      "- **Best: combine 1+2** (AMEN method).  Instead of optimizing $A \\approx UV^{\\top}$ we enrich the basis with new elements.  \n",
      "All of them are implemented in the Toolboxes (both Python and MATLAB)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Linear system demo"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tt\n",
      "from tt.amen import amen_solve\n",
      "\n",
      "d = 8 \n",
      "f = 4\n",
      "mat = tt.qlaplace_dd([d] * f)\n",
      "rhs = tt.ones(2, d * f)\n",
      "sol = amen_solve(mat, rhs, rhs, 1e-6)\n",
      "print 'Error:', (tt.matvec(mat, sol) - rhs).norm()/rhs.norm()\n",
      "print 'Efficient rank:', sol.erank"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5.96702346748e-07\n",
        "Efficient rank: 35.2790975909\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Recent results\n",
      "- Rectangular maximum-volume principle (A. Mikhalev): selecting representative columns/rows\n",
      "- Dynamical low-rank approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Rectangular maximum volume\n",
      "Maximum volume-principle for square matrices:  \n",
      "\n",
      "If $A$ is $n \\times r$, and $\\widehat{A}$ is the submatrix with the largest volume, then  \n",
      "\n",
      "$$A = C \\widehat{A},$$\n",
      "\n",
      "where $|C_{ij}| \\leq 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Rectangular maximum volume (2)\n",
      "Maximum volume-principle for rectangular matrices:  \n",
      "\n",
      "If $A$ is $n \\times r$, and $\\widehat{A}$ is the submatrix $K \\times r, \\quad K \\geq r$ with the **largest 2-volume**, then  \n",
      "\n",
      "$$A = C \\widehat{A},$$\n",
      "\n",
      "where $\\Vert C_i \\Vert_2 \\leq \\sqrt{\\frac{r}{K + r - 1}}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Volume of rectangular matrices\n",
      "\n",
      "We define $p$-volume of a matrix $K \\times r$ matrix $A$ with rows $a_i, i = 1, \\ldots, K$, \n",
      "as the volume of the set  \n",
      "\n",
      "$$G = \\{ x: x = \\sum_i c_i a_i, \\quad \\Vert c \\Vert_p \\leq 1 \\}$$  \n",
      "\n",
      "The 2-volume:\n",
      "\n",
      "$$\\mathrm{vol}(G) = \\gamma \\det A^* A$$\n",
      "\n",
      "The 2-volume can be efficiently optimized, leading to the **rectmaxvol** algorithm.  \n",
      "\n",
      "Typically, $K \\approx 2r$. \n",
      "\n",
      "https://bitbucket.org/muxas/rect_maxvol  (Alexander Mikhalev)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Applications of rectmaxvol\n",
      "- TT-cross approximation (used to be: select $r$ rows from $rn \\times r$ matrix -- no rank adaptation).\n",
      "- Recommender systems\n",
      "- Preconditioning of linear least squares\n",
      "- Finding maximal elements in low-rank matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Solving dynamical problems\n",
      "Besides solving optimization (or rank-constrained problems) we can also solve **dynamical problems**, i.e.  \n",
      "\n",
      "efficiently **recompute** decompositions when matrix/tensor change"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Concept of dynamical low-rank approximation\n",
      "\n",
      "We have a time-varying matrix $A(t)$ (time can be discrete!) and we want to do low-rank approximation of it:  \n",
      "\n",
      "$$A(t) \\approx X(t) = U(t) S(t) V^{\\top}(t).$$\n",
      "\n",
      "Naive method (SVD projection) ignores time-dependence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Dirac-Frenkel variational principle\n",
      "\n",
      "In quantum physics, for many years an optimal solution is defined via **Dirac-Frenkel principle**:  \n",
      "\n",
      "$$\\Vert \\frac{dA}{dt} - \\frac{dX}{dt} \\Vert = \\min,$$  \n",
      "\n",
      "or\n",
      "\n",
      "$$\n",
      "    \\langle \\frac{dA}{dt} - \\frac{dX}{dt}, Z \\rangle = 0 \n",
      "$$\n",
      "for all $Z$ in the **tangent space** to the manifold\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Equations of motion in 2D case\n",
      "(Lubich, Koch, H.-D. Meyer,...)  \n",
      "\n",
      "$$\\begin{split}\n",
      "&\\frac{dU}{dt} S = \\frac{dA}{dt} V,\\\\\n",
      "&\\frac{dV}{dt} S^{\\top} = \\frac{dA^{\\top}}{dt} U, \\\\\n",
      "&\\frac{dS}{dt} = U^{\\top} \\frac{dA}{dt} V.\n",
      "\\end{split}$$\n",
      "Suppose, you start from rank-$1$ and evolve into rank-$10$ case: the matrix $S$ will be singular!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Projector-splitting\n",
      "$$\\frac{dX}{dt} = P_X(\\frac{dA}{dt}), $$\n",
      "where\n",
      "$$\n",
      "  P_X(Z) = Z - (I - UU^{\\top}) Z (I - VV^{\\top}).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Splitting integrator\n",
      "The final splitting steps are\n",
      "- **Step \"L\":**  $$\\begin{split}L_0 = V_0 S^{\\top}_0 \\\\ L_1 = L_0 + (A_1 - A_0)^{\\top} U_0 \\\\ \n",
      "       [V_1, S^{\\top}_1] = QR(L_1)\\end{split}$$\n",
      "- **Step \"K\":** $$\\begin{split} K_0 = U_0 S^{\\top}_0 \\\\ K_1 = K_0 + (A_1 - A_0) V_0 \\\\ \n",
      "       [U_1, S_1] = QR(K_1)\\end{split}$$\n",
      "\n",
      "- **Step \"L\":** $$S_1 = S_0 - U^{\\top}_0 (A_1 - A_0) V_0$$ (note the minus!)\n",
      "The order of the steps matters  \n",
      "\n",
      "Can be used for updating!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Application to convolutional neural networks\n",
      "[Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky,arxiv 1412.6553](http://arxiv.org/abs/1412.6553)\n",
      "\n",
      "We used tensors to speed up CNN with a factor of 10. \n",
      "\\begin{equation}\n",
      "V(x,y,t) = \\sum_{i=x-\\delta}^{x+\\delta} \\; \\sum_{j=y-\\delta}^{y+\\delta} \\; \\sum_{s=1}^S K(i,j,s,t)\\, U(i,j,s)\n",
      "\\end{equation}\n",
      "\n",
      "We approximate $K$ by a CP-decomposition and get an excellent initial condition for the fine-tuning!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## CNN: results\n",
      "36-chacter CNN: 1% accuracy drop and 8.5x speedup  \n",
      "\n",
      "AlexNet: speedup second convolution layer by a factor of 4, 1% increase of top-5 error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Conclusions\n",
      "- There are several SVD-based tensor formats\n",
      "- We can solve rank-constrained optimization/dynamical/interpolation problems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Publications, software\n",
      "- http://oseledets.github.io - group website, papers, etc.\n",
      "- http://github.com/oseledets/TT-Toolbox\n",
      "- http://github.com/oseledets/ttpy\n",
      "- https://bitbucket.org/muxas/rect_maxvol"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##### Questions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"../common/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "html": [
        "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
        "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
        "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        /*width:80%;*/\n",
        "        /*margin-left:auto !important;\n",
        "        margin-right:auto;*/\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: 'Alegreya Sans', sans-serif;\n",
        "    }\n",
        "    h2 {\n",
        "        font-family: 'Fenix', serif;\n",
        "    }\n",
        "    h3{\n",
        "\t\tfont-family: 'Fenix', serif;\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "\th4{\n",
        "\t\tfont-family: 'Fenix', serif;\n",
        "       }\n",
        "    h5 {\n",
        "        font-family: 'Alegreya Sans', sans-serif;\n",
        "    }\t   \n",
        "    div.text_cell_render{\n",
        "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        font-size: 160%;\n",
        "        line-height: 121%;\n",
        "        /*width:70%;*/\n",
        "        /*margin-left:auto;*/\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    li {\n",
        "        line-height: 121%;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\";\n",
        "\t\t\tfont-size: 90%;\n",
        "    }\n",
        "/*    .prompt{\n",
        "        display: None;\n",
        "    }*/\n",
        "    .text_cell_render h1 {\n",
        "        font-weight: 200;\n",
        "        font-size: 50pt;\n",
        "\t\tline-height: 110%;\n",
        "        color:#CD2305;\n",
        "        margin-bottom: 0.5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\t\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #CD2305;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        ".floated_img\n",
        "{\n",
        "        float: left;\n",
        "    }\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<IPython.core.display.HTML at 0x104364b10>"
       ]
      }
     ],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}