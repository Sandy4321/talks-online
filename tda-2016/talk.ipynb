{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.paths import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})\n",
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import tt\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import scipy.sparse as ssp\n",
    "import scipy.sparse.linalg as sla\n",
    "import math\n",
    "\n",
    "#Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "#import prettyplotlib as ppl\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "import matplotlib as mpl\n",
    "rc_cmufonts = {\"font.family\": \"normal\", \n",
    "                \"font.serif\": [], \n",
    "                \"font.sans-serif\": [], \n",
    "                \"font.monospace\": []}\n",
    "\n",
    "fs = 12\n",
    "font = {'size' : fs}\n",
    "mpl.rc('font', **font)\n",
    "mpl.rcParams.update(rc_cmufonts)\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convergence on low-rank manifolds\n",
    "##### Ivan Oseledets\n",
    "##### Skolkovo Institute of Science and Technology\n",
    "##### Based on joint work with C. Lubich, H. Walach, D. Kolesnikov\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The topic of this talk\n",
    "\n",
    "Recently, much attention has been paid to solution of optimization problems on **low-rank matrix and tensor manifolds.**\n",
    "\n",
    "- Low-rank matrix manifold: $A \\in \\mathbb{R}^{n \\times m}$, $A = UV^{\\top}$.\n",
    "- Low-rank **tensor-train** manifold, $\\mathrm{rank}~A_k = r_k.$\n",
    "\n",
    "These methods are really important, since they lead to huge complexity reduction, and there are recent theoretical results for the approximability of solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problems examples\n",
    "\n",
    "- Linear systems: $F(X) = \\langle A(X), X \\rangle - 2 \\langle F, X\\rangle.$\n",
    "- Eigenvalue problems $F(X) = \\langle A(X), X \\rangle, \\mbox{s.t.} \\Vert X \\Vert = 1.$\n",
    "- Tensor completion: $F(X) = \\Vert W \\circ (A  - X)\\Vert.$\n",
    "\n",
    "My claim is that we need better understanding of how these methods converge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General scheme\n",
    "The general scheme is that we have some (convergent method) in a full space:\n",
    "\n",
    "$$X_{k+1} = \\Phi(X_k),$$\n",
    "\n",
    "and we know that the solution $X_*$ is on the manifold $\\mathcal{M}$.\n",
    "\n",
    "Then we introduce a **Riemannian projected method**\n",
    "\n",
    "$$X_{k+1} = R(X_k + P_{\\mathcal{T}} \\Phi(X_k)),$$\n",
    "\n",
    "where $P_{\\mathcal{T}}$ is a projection on the **tangent space**, and $R$ is a **retraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riemannian optimization is easy\n",
    "\n",
    "The projection onto the tangent space is trivial for low-rank (and is not difficult for TT)\n",
    "\n",
    "$$P_{T}(X) = X - (I - UU^{\\top}) X (I - VV^{\\top}).$$\n",
    "\n",
    "For the retraction, there **many choices** (see the review Absil, O., 2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projector-splitting scheme \n",
    "- One of the simplest **second-order retractions** is the projector-splitting (or KSL) scheme \n",
    "\n",
    "- Initially proposed as a time integrator for the dynamical low-rank approximation (C. Lubich, I. Oseledets, A projector-splitting... 2014) for matrices and (C. Lubich, I. Oseledets, B. Vandreycken, Time integration of tensor trains, SINUM, 2015).\n",
    "\n",
    "- Reformulated as a retraction is (Absil, Oseledets)\n",
    "\n",
    "Has a trivial form: a half-step (or full) step of the Alternating least squares (ALS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The simplest retraction possible:\n",
    "\n",
    "$$ U_1, S_1 = \\mathrm{QR}(A V_0),  \\quad V_1, S_2^{\\top} = \\mathrm{QR}(A^{\\top} U_1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projection onto the tangent space not needed!\n",
    "\n",
    "$$X_{k+1} = I(X_k, F), $$\n",
    "\n",
    "where $I$ the is the **projector-splitting** integrator, and $F$ is the step for the full method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about the convergence?\n",
    "\n",
    "Now, instead of $$X_{k+1} = \\Phi(X_k), \\quad X_* = \\Phi(X_*),$$\n",
    "\n",
    "and $X_*$ is on the manifold, we have the **manifold-projected** process\n",
    "\n",
    "$$Y_{k+1} = I(Y_k, \\Phi(Y_k) - Y_k).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear manifold\n",
    "Suppose the linear case,\n",
    "$$\\Phi(X) = X + Q(X), \\quad \\Vert Q \\Vert < 1.$$\n",
    "\n",
    "and $M$ is a **linear subspace**. Then the projected method is always **not slower.**\n",
    "\n",
    "On a curved manifold, the **curvature** of the manifold plays crucial role in the convergence estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curvature of fixed-rank manifold\n",
    "\n",
    "The curvature of the manifold of matrices with rank $r$ at point $X$ is equal to the $$\\Vert X^{-1} \\Vert_2,$$\n",
    "\n",
    "i.e. the inverse of the minimal singular value.\n",
    "\n",
    "\n",
    "**In practice we know,** that zero singular values for the **best rank-r** approximation **do not harm:**\n",
    "\n",
    "\n",
    "Approximation of a rank-$1$ matrix with a rank-$2$ matrix is ok (block power method.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experimental convergence study\n",
    "\n",
    "Our numerical experiments confirm that the low-rank matrix manifold behaves typically like a **linear manifold**, i. e. \n",
    "\n",
    "**projected gradient** is almost always faster, independent of the curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projection-splitting as a projection onto the \"middle point\".\n",
    "\n",
    "**Lemma.**  We can write down one step of the projector-splitting scheme as a projection onto the middle point:\n",
    "\n",
    "$$X_1 = I(X_0, F) = P_{\\mathcal{T}(X_m)}( X_0 + F).$$\n",
    "\n",
    "The proof is simple. Let $X_0 = U_0 S_0 V^{\\top}_0$ and $X_1 = U_1 S_1 V^{\\top}_1.$\n",
    "\n",
    "Then take any matrix of the form $U_1, V_0$ as the **middle point**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decomposition of the error\n",
    "\n",
    "We can decompose the error into the **normal component** and the **tangent component.**\n",
    "\n",
    "Indeed, consider one step.\n",
    "$$Y_1 = I(Y_0, F), \\quad Y_0 + F = X_* + H, \\quad \\Vert H \\Vert \\leq \\delta \\Vert Y_0 - X_* \\Vert.$$\n",
    "Then, $$Y_1 = P(Y_0 + F) = P(X_* + H).$$\n",
    "We have $$E_1 = Y_1 - X_* = P(X_* + H) - X_* = -P_{\\perp}(X_*) + P(H).$$\n",
    "This is the decomposition of the error into the **tangent** and **normal** components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#2d case functions\n",
    "def grad(A, x, x0):\n",
    "    #u, s, v = x\n",
    "    #u0, s0, v0 = x0\n",
    "    #u_new = np.linalg.qr(np.hstack((u, u0)))[0]\n",
    "    #v_new = np.linalg.qr(np.hstack((v, v0)))[0]\n",
    "    #s_new = u_new.T.dot(u).dot(s).dot(v.T.dot(v_new)) - u_new.T.dot(u0).dot(s0).dot(v0.T.dot(v_new))\n",
    "    return x0 - A.dot(full(x).flatten()).reshape(x0.shape)\n",
    "    #return (u_new, s_new, v_new)\n",
    "\n",
    "\n",
    "#it is Frobenius norm\n",
    "def get_norm(x):\n",
    "    u, s, v = x\n",
    "    return la.norm(s)\n",
    "    #return math.sqrt(np.trace((u.T.dot(u)).T.dot(v.T.dot(v))))\n",
    "    \n",
    "    \n",
    "def check_orthogonality(u): \n",
    "    if la.norm(u.T.dot(u) - np.eye(u.shape[1])) / math.sqrt(u.shape[1]) < 1e-12:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def orthogonalize(x):\n",
    "    u, s, v = x\n",
    "    u_new, ru = np.linalg.qr(u)\n",
    "    v_new, lv = np.linalg.qr(u)\n",
    "    s_new = ru.dot(s.dot(lv))\n",
    "    return (u_new, s_new, v_new)\n",
    "\n",
    "\n",
    "def diagonalize_core(x):\n",
    "    u, s, v = x\n",
    "    ls, s_diag, rs = la.svd(s)\n",
    "    return (u.dot(ls), np.diag(s_diag), v.dot(rs))\n",
    "    \n",
    "\n",
    "def func(x, x0):\n",
    "    return get_norm(grad(x, x0))\n",
    "\n",
    "    \n",
    "def full(dx):\n",
    "    return dx[0].dot(dx[1].dot(dx[2].T))\n",
    "\n",
    "\n",
    "def projector_splitting_2d(x, dx, flag_dual=False):\n",
    "    n, r = x[0].shape\n",
    "    u, s, v = x[0].copy(), x[1].copy(), x[2].copy()\n",
    "    if not flag_dual:\n",
    "        u, s = np.linalg.qr(u.dot(s) + dx.dot(v))\n",
    "        s = s - u.T.dot(dx).dot(v)\n",
    "        v, s = np.linalg.qr(v.dot(s.T) + dx.T.dot(u))\n",
    "        s = s.T\n",
    "    else:\n",
    "        v, s = np.linalg.qr(v.dot(s.T) + dx.T.dot(u))\n",
    "        s = s.T\n",
    "        s = s - u.T.dot(dx).dot(v)\n",
    "        u, s = np.linalg.qr(u.dot(s) + dx.dot(v))\n",
    "        \n",
    "    return u, s, v\n",
    "\n",
    "\n",
    "def inter_point(x, dx):\n",
    "    u, s, v = x\n",
    "    u, s = np.linalg.qr(u.dot(s) + dx.dot(v))\n",
    "    #dx - (I - uu') dx (I - vv') = uu' dx + dx * vv' - uu' dx vv'\n",
    "    dx_tangent = u.dot(u.T.dot(dx)) + (dx.dot(v)).dot(v.T) - u.dot(u.T.dot(dx).dot(v)).dot(v.T)\n",
    "    \n",
    "    return u.copy(), v.copy(), dx_tangent\n",
    "\n",
    "\n",
    "def minus(x1, x2):\n",
    "    u1, s1, v1 = x1\n",
    "    u2, s2, v2 = x2\n",
    "    u_new = np.linalg.qr(np.hstack((u1, u2)))[0]\n",
    "    v_new = np.linalg.qr(np.hstack((v1, v2)))[0]\n",
    "    s_new = u_new.T.dot(u1).dot(s1).dot(v1.T.dot(v_new)) - u_new.T.dot(u2).dot(s2).dot(v2.T.dot(v_new))\n",
    "    return u_new, s_new, v_new\n",
    "\n",
    "\n",
    "def ps_proj(x, dx):\n",
    "    return full(projector_splitting_2d(x, dx)) - full(x)\n",
    "\n",
    "def rotation(u):\n",
    "    return la.qr(u)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear test case\n",
    "\n",
    "$$\\Phi (X) = X + A(X) - f,$$\n",
    "\n",
    "where $A$ is a linear operator on matrix space, $\\Vert A - I\\Vert = \\delta$, \n",
    "\n",
    "$f$ is known right-hand side and $X_*$ is the solution of linear equation $A(X_*) = f$.\n",
    "\n",
    "This problem is equivalent to the minimization problem of the quadratic functional $F(X) = \\langle A(X) - f, X \\rangle.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up the experiment\n",
    "\n",
    "Generate some random matrices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Init sizes\n",
    "n, r, r0 = 40, 7, 7\n",
    "\n",
    "M = n * n \n",
    "Q = np.random.randn(M, M)\n",
    "Q = Q + Q.T\n",
    "Q = (Q/np.linalg.norm(Q, 2)) * 0.8 # contraction coefficient\n",
    "A = np.eye(M) + Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic case\n",
    "In the generic case, the convergence is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Case 1: Projector-Splitting versus Gradient descent\n",
    "#Random initialization\n",
    "x_orig = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start, x_orig = orthogonalize(x_start), orthogonalize(x_orig)\n",
    "x_orig = diagonalize_core(x_orig)\n",
    "print 'The singular values of fixed point matrix are \\n', np.diag(x_orig[1])\n",
    "f = full(x_orig)\n",
    "f = A.dot(f.flatten()).reshape(f.shape)\n",
    "grad_dist, orth_proj_norm, tangent_proj_norm = [], [], []\n",
    "k = 50\n",
    "# Gradient Descent Convergence\n",
    "x = full(x_start)\n",
    "for i in xrange(k):\n",
    "    grad_dist.append(la.norm(x - full(x_orig)))\n",
    "    dx = f - (A.dot(x.flatten())).reshape(x.shape)\n",
    "    x = x + dx\n",
    "# Projector Splitting Convergence\n",
    "x = x_start\n",
    "dx_orig = full(x)-full(x_orig)\n",
    "for i in xrange(k):\n",
    "    dx = grad(A, x, f)\n",
    "    u1, v, dx_tangent = inter_point(x, dx_orig)\n",
    "    dx_orig = full(x)-full(x_orig)\n",
    "    dx_orig_tangent = u1.dot(u1.T.dot(dx_orig)) + (dx_orig.dot(v)).dot(v.T) - u1.dot(u1.T.dot(dx_orig).dot(v)).dot(v.T)\n",
    "    orth_proj_norm.append(la.norm(dx_orig - dx_orig_tangent))\n",
    "    tangent_proj_norm.append(la.norm(dx_orig_tangent))\n",
    "    x = projector_splitting_2d(x, dx)\n",
    "# Plotting\n",
    "plt.semilogy(grad_dist, marker='x', label=\"GD\")\n",
    "plt.semilogy(orth_proj_norm, marker='o', label=\"Orthogonal projection\")\n",
    "plt.semilogy(tangent_proj_norm, marker='o', label=\"Tangent projection\")\n",
    "plt.legend(bbox_to_anchor=(0.40, 1), loc=2)#(1.05, 1), loc=2\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad curvature \n",
    "\n",
    "Now let us design bad singular values for the solution,\n",
    "\n",
    "$$\\sigma_k = 10^{2-2k}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Case 2: Stair convergence\n",
    "x_orig = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start, x_orig = orthogonalize(x_start), orthogonalize(x_orig)\n",
    "x_orig = diagonalize_core(x_orig)\n",
    "u, s, v = x_orig\n",
    "s_diag = [10**(2-2*i) for i in xrange(r)]\n",
    "x_orig = (u, np.diag(s_diag), v)\n",
    "print 'The singular values of fixed point matrix are \\n', s_diag\n",
    "f = full(x_orig)\n",
    "f = A.dot(f.flatten()).reshape(f.shape)\n",
    "grad_dist, orth_proj_norm, tangent_proj_norm = [], [], []\n",
    "k = 50\n",
    "# Gradient Descent Convergence\n",
    "x = full(x_start)\n",
    "for i in xrange(k):\n",
    "    grad_dist.append(la.norm(x - full(x_orig)))\n",
    "    dx = f - (A.dot(x.flatten())).reshape(x.shape)\n",
    "    x = x + dx\n",
    "# Projector Splitting Convergence\n",
    "x = x_start\n",
    "for i in xrange(k):\n",
    "    dx = grad(A, x, f)\n",
    "    u1, v, dx_tangent = inter_point(x, dx_orig)\n",
    "    dx_orig = full(x)-full(x_orig)\n",
    "    dx_orig_tangent = u1.dot(u1.T.dot(dx_orig)) + (dx_orig.dot(v)).dot(v.T) - u1.dot(u1.T.dot(dx_orig).dot(v)).dot(v.T)\n",
    "    orth_proj_norm.append(la.norm(dx_orig - dx_orig_tangent))\n",
    "    tangent_proj_norm.append(la.norm(dx_orig_tangent))\n",
    "    x = projector_splitting_2d(x, dx)\n",
    "# Plotting\n",
    "plt.semilogy(grad_dist, marker='x', label=\"GD\")\n",
    "plt.semilogy(orth_proj_norm, marker='o', label=\"Orthogonal projection\")\n",
    "plt.semilogy(tangent_proj_norm, marker='o', label=\"Tangent projection\")\n",
    "plt.legend(bbox_to_anchor=(0.40, 1), loc=2)#(1.05, 1), loc=2\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consequences\n",
    "\n",
    "The typical convergence: \n",
    "\n",
    "- Tangent component convergence linearly.\n",
    "- Normal components decays quadratically until it hits **the next singular values**, and then waits for the tangent component to catch. \n",
    "- The convergence is **monotone:** First the first singular vector converge, then the second and so on.\n",
    "\n",
    "Adversal examples are still possible (similar to the convergence of the power method).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case when Riemannian is worse\n",
    "You can make up an example, when on the first iteration it is worse: it is basically the **angle** between $V_*$ and $V_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Case 3: Stair convergence\n",
    "x_orig = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start = np.random.randn(n, r0), np.random.randn(r0, r0), np.random.randn(n, r0)\n",
    "x_start, x_orig = orthogonalize(x_start), orthogonalize(x_orig)\n",
    "eps = 1e-12\n",
    "u, s, v = x_orig\n",
    "u1, s1, v1 = x_start\n",
    "u1 = u1 - u.dot(u.T.dot(u1)) \n",
    "v1 = v1 - v.dot(v.T.dot(v1))\n",
    "x_start = u1, s1, v1\n",
    "x_orig = diagonalize_core(x_orig)\n",
    "print 'The singular values of fixed point matrix are \\n', s_diag\n",
    "f = full(x_orig)\n",
    "f = A.dot(f.flatten()).reshape(f.shape)\n",
    "grad_dist, proj_dist = [], []\n",
    "k = 10\n",
    "# Gradient Descent Convergence\n",
    "x = full(x_start)\n",
    "for i in xrange(k):\n",
    "    grad_dist.append(la.norm(x - full(x_orig)))\n",
    "    dx = f - (A.dot(x.flatten())).reshape(x.shape)\n",
    "    x = x + dx\n",
    "# Projector Splitting Convergence\n",
    "x = x_start\n",
    "for i in xrange(k):\n",
    "    dx = grad(A, x, f)\n",
    "    u1, v, dx_tangent = inter_point(x, dx_orig)\n",
    "    dx_orig = full(x)-full(x_orig)    \n",
    "    proj_dist.append(la.norm(dx_orig))\n",
    "    x = projector_splitting_2d(x, dx)\n",
    "# Plotting\n",
    "plt.semilogy(grad_dist, marker='x', label=\"GD\")\n",
    "plt.semilogy(proj_dist, marker='o', label=\"Projector Splitting\")\n",
    "plt.legend(bbox_to_anchor=(0.40, 1), loc=2)#(1.05, 1), loc=2\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory status\n",
    "\n",
    "We are still working on the theory.\n",
    "\n",
    "Even the case $n = 2$, $r = 1$ is non-trivial, but yesterday (Tuesday 19th) the estimate was obtained.\n",
    "\n",
    "Actually, I think it can be generalized to arbitrary $n, r$ by block matrix argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "- For the algorithms: this \"gradual convergence\" may give a hint on how to adapt the ranks during the iteration.\n",
    "- For applications, they are many (and we are working on machine learning applications, stay tuned).\n",
    "- The TT-case is in progress as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Papers and software\n",
    "**Software**\n",
    "- TT-Toolbox: https://github.com/oseledets/ttpy (Python)\n",
    "- https://github.com/oseledets/TT-Toolbox (MATLAB)\n",
    "**Papers**\n",
    "- Christian Lubich and Ivan V. Oseledets. A projector-splitting integrator for dynamical low-rank approximation. BIT, 54(1):171–188, 2014\n",
    "- [Christian Lubich, Ivan Oseledets, and Bart Vandereycken. Time integration of tensor trains. arXiv preprint 1407.2042, 2014.](http://arxiv.org/abs/1407.2042), published in SINUM\n",
    "- [Jutho Haegeman, Christian Lubich, Ivan Oseledets, Bart Vandereycken, and Frank Verstraete. Unifying time evolution and optimization with matrix product states. arXiv preprint 1408.5056, 2014.](http://arxiv.org/abs/1408.5056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
