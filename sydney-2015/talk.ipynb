{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensor trains: generalization of the SVD to many dimensions\n",
    "\n",
    "##### Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 near Moscow http://faculty.skoltech.ru\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens \n",
    "- I work at Center for Computational Data-Intensive Science and Engineering http://crei.skoltech.ru/cdise/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CDISE  at Skoltech\n",
    "- [Ivan Oseledets](http://faculty.skoltech.ru/Faculty/Ivan-Oseledets): tensors, multidimensional approximations, shape & topology optimization\n",
    "- [Victor Lempitsky](http://faculty.skoltech.ru/Faculty/Victor-Lempitsky): computer vision, deep learning\n",
    "- [Dmitry Vetrov](http://bayesgroup.ru/): machine learning, deep learning\n",
    "- [Panos Karras](http://faculty.skoltech.ru/Faculty/Panagiotis-Karras): data management, data science\n",
    "- [Thanos Polimeridis](http://faculty.skoltech.ru/Faculty/Athanasios-Polimeridis), Maxwell equation, PDEs\n",
    "- [Alexander Shapeev](http://faculty.skoltech.ru/Faculty/Alexander-Shapeev), PDEs, multiscale optimizatio\n",
    "- [Dzmitry-Tsetserukou](http://faculty.skoltech.ru/Faculty/Dzmitry-Tsetserukou), robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Courses\n",
    "- We teach Computational Science & Data Science (NLA, PDEs, Comp Vision, Deep Learning)\n",
    "- We welcome new faculty, postdocs, PhD students, master students. We are hiring!\n",
    "- We are open to collaboration with everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the talk\n",
    "- Motivation for really high-dimensional tensors in image processing\n",
    "- Tensor-train format\n",
    "- Optimization over low-rank manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This talk\n",
    "This talk is about **tensors**. \n",
    "\n",
    "A tensor is just a multidimensional array\n",
    "\n",
    "$$A(i_1, \\ldots, i_d), \\quad 1 \\leq i_k \\leq n_k.$$\n",
    "\n",
    "Suppose that $n_k \\approx n$, \n",
    "the the total amount of memory to be stored is $n^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The core idea\n",
    "Numerical tensor methods have many applications in machine learning, PDEs, data mining.\n",
    "\n",
    "**In one sentence, it is a generalization of standard linear algebra (singular value decomposition)**  \n",
    "\n",
    "to multidimensional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need may dimensions?\n",
    "\n",
    "But for image processing, where the tensors come from? \n",
    "\n",
    "Do we really need so many dimensions?\n",
    "\n",
    "A color image is a 3D array, a video is a 4D array, but we need more.\n",
    "\n",
    "Consider a $512 \\times 512$ grayscale image. Can we make a tensor out of it?\n",
    "\n",
    "<p>\n",
    "\n",
    "<font color='red'> Introduce virtual dimensions! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1D case\n",
    "\n",
    "A signal, $f(x)$, sampled on a uniform grid with $2^d$ points, **reshape it** into a $2 \\times \\ldots \\times 2$ $d$-dimensional tensor\n",
    "<p>    </p>\n",
    "$$V(i_1, \\ldots, i_d) = v(i_1 + 2 i_2+ \\ldots+2^{d-1} i_d)$$\n",
    "\n",
    "For example, an exponential signal: $f(x) = \\exp(\\lambda x)$ breaks into the product of vectors of length $2$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D case\n",
    "First we tensorize indices, then we permute them to bring $i_k j_k$ together.\n",
    "\n",
    "$$A(i_1 i_2 \\ldots i_d; j_1 \\ldots j_d) \\rightarrow B(i_1 j_1; i_2 j_2; \\ldots i_d j_d), \\mbox{Hilbert curve ordering}$$\n",
    "\n",
    "Why do we do it, it another question: our goal is to introduce a **reasonable structure** for image representation via tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Images -> Tensors\n",
    "Now we have a way to make high-dimensional tensors. But how do we compress them? How we use it in image processing tasks?\n",
    "\n",
    "We need compact representations, i.e. **tensor decompositions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor decompositions\n",
    "The idea of tensor decomposition is to represent a tensor (multidimensional array) in terms of simpler array.\n",
    "\n",
    "\n",
    "This idea comes from matrix factorizations (represent a matrix as a product  of simpler ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the simplest tensor decomposition\n",
    "The key idea is the idea of **separation of variables**:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = U_1(i_1) \\ldots U_d(i_d)$$.\n",
    "\n",
    "In probability theory, it corresponds to the idea of **independence** of variables.\n",
    "\n",
    "This class is too **narrow** to represent useful functions (or probability distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sum-of-products\n",
    "The sum-of-products representation is much more interesting:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "also named **canonical format**, or **multivariate factor model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two questions\n",
    "- What this decomposition is for $d=2$.\n",
    "- What is good and/or bad for $d \\geq 3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two-dimensional case\n",
    "For two-dimensional case, it boils down to the approximation\n",
    "$$A \\approx UV^{\\top}, $$\n",
    "where $U$ is $n \\times r$ and $V$ is $m \\times r$.\n",
    "\n",
    "It is **low-rank approximation** of matrices, and best rank-$r$ approximation can be computed  \n",
    "\n",
    "using **singular value decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "\n",
    "For $d > 3$\n",
    "The canonical decomposition of the form\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "then it is **very difficult** to compute numerically; there is even an example of $9 \\times 9 \\times 9$ \n",
    "\n",
    "tensor for which the tensor rank is not known!\n",
    "\n",
    "**But this is not the only tensor decomposition** we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Literature on tensor formats\n",
    "- T. Kolda and B. Bader, Tensor decompositions and applications, SIREV (2009)\n",
    "- W. Hackbusch, Tensor spaces and numerical tensor calculus, 2012\n",
    "- L. Grasedyck, D. Kressner, C. Tobler,\n",
    " A literature survey of low-rank tensor approximation techniques, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "Some times, it is better to study with the code and to try your own application\n",
    "- Tensor Toolbox 2.5 (T. Kolda)\n",
    "- TT-Toolbox, MATLAB (http://github.com/oseledets/TT-Toolbox)\n",
    "- TT-Toolbox, Python (http://github.com/oseledets/ttpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic stuff\n",
    "Now we will briefly recall the two-dimensional case, which is important for the techniques for the computational of tensor decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "Given an $n \\times m$ matrix $A$, its **singular value decomposition** (SVD) is defined as \n",
    "\n",
    "$$A = U S V^{\\top},$$\n",
    "where $U^*U = V^* V = I$, and $S$ contains **singular values** on the diagonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computation of the SVD\n",
    "\n",
    "Standard algorithm for the computation of the SVD requires $\\mathcal{O}(N^3)$ operations,  \n",
    "can be done for matrices $1000-5000$ on a laptop.\n",
    "\n",
    "**Large-scale SVD** is a topic of ongoing research.  \n",
    "\n",
    "The crucial point why it is possible is based on the **skeleton decomposition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "\n",
    "Given a rank-$r$ matrix, how many elements you need to compute to recover the full matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "It is enough to sample $r$ columns and $r$ rows to exactly recover a rank-$r$ matrix:\n",
    "\n",
    "$$A = U \\widehat{A}^{-1} V,$$\n",
    "\n",
    "<img src=\"cross-pic.png\" \\img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Approximate rank-r\n",
    " What happens if the matrix is of approximate low rank?  \n",
    " \n",
    " \n",
    " $A \\approx R + E, \\quad \\mathrm{rank~} R = r, \\quad ||E||_C = \\varepsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum-volume principle\n",
    "\n",
    "Answer is given by the **maximum-volume** principle: select the columns and rows such that  \n",
    "\n",
    "the submatrix $\\widehat{A}$ has maximal-volume among all submatrices, then **quasi-optimality** holds:\n",
    "\n",
    "$$\\Vert A - A_{skel} \\Vert \\leq (r+1)^2 \\Vert A - A_{best} \\Vert.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to earth: applications of maxvol\n",
    "\n",
    "The maximal-volume principle is present in many areas, one of the earliest related references I know is the **Design-optimality** (Fedorov, 1972), and is related to the selection of **optimal interpolation points** for a given basis set:\n",
    "\n",
    "$$f(x) \\approx \\sum_{\\alpha=1}^r c_{\\alpha} g_{\\alpha}(x),$$\n",
    "\n",
    "instead of doing linear regression, just do interpolation at $r$ points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some more ideas\n",
    "- Machine learning tasks: given millions of features, select **most important** features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear algebra showcase\n",
    "\n",
    "Given an $n \\times r$ matrix, find the submatrix of largest **volume** it in.\n",
    "\n",
    "Use greedy algorithm (now we know that it is D-optimality from at least 1972)\n",
    "- Take some rows, put them in the first $r$. Compute $B = A \\widehat{A}^{-1}$\n",
    "- $B = \\begin{pmatrix} I \\\\\n",
    "    Z \\end{pmatrix}$\n",
    "- Suppose maximal element in $Z$ is in position $(i,j)$. \n",
    "- Swap $i$-th row with $j$-th row.\n",
    "- Stop if maximal element is less than $(1+\\delta)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key idea\n",
    "Select subsets of raw data, that describe the behaviour sufficiently well (**interpolation problem**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation\n",
    "For the two-dimensional case, the **best algorithm** to recover the rank-$r$ approximation is  \n",
    "the cross approximation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is it better than random sampling of columns?\n",
    "Randomized techniques for low-rank approximation became popular\n",
    "recently, but no **real comparisons** are done (Martinsson, Halko, Rokhlin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional case\n",
    "How to generalize the idea of separation of variables to higher\n",
    "dimensions?\n",
    "\n",
    "\n",
    "#### Important points\n",
    "\n",
    "- SVD is good\n",
    "- Best approximation exists\n",
    "- Interpolation via skeleton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Straightforward idea: canonical format\n",
    "\n",
    "\n",
    "$A(i_1,\\ldots,i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1,\\alpha)\\ldots\n",
    "U_d(i_d,\\alpha)$  \n",
    "\n",
    "$r$ is called (approximate) canonical rank, $U_k$ --- canonical\n",
    "factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bad things about the canonical format:\n",
    "- Best approximation may not exist\n",
    "- Canonical rank is NP-complete (matrix rank is ...)\n",
    "- No good algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  ## Bad example (1):\n",
    "  $f(x_1,\\ldots,x_d) = x_1 + x_2 + \\ldots x_d$, \n",
    "  \n",
    "  Canonical rank $d$ (no proof is known), can be\n",
    "  approximated with rank-$2$ with any accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad example (2):\n",
    "Canonical rank may depend on the field (matrix rank can not!)\n",
    "\n",
    " $f(x_1,\\ldots,x_d) = \\sin(x_1 + \\ldots + x_d)$ \n",
    "- Complex field: 2 (obvious!)\n",
    "- Real field: $d$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example: Tucker\n",
    "Another attempt to avoid was the Tucker format \n",
    "\n",
    "(Tucker 1966, Lathauwer, 2000+), used first in statistics, where the data was given as a **data cube**.\n",
    "\n",
    "$$A(i,j,k) \\approx \\sum_{\\alpha \\beta \\gamma} G(\\alpha,\\beta,\\gamma)\n",
    "U_1(i,\\alpha)V(j,\\alpha)W(k,\\alpha)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can compute Tucker by means of the SVD:\n",
    "- Compute <font color='red'> unfoldings </font>: $A_1, A_2, A_3$ \n",
    "- Compute left SVD factors: $A_i \\approx U_i \\Phi_i$\n",
    "- Compute the core: $G = A \\times_1 U^{\\top}_1 \\times_2 U^{\\top}_2\n",
    "  \\times_3 U^{\\top}_3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main problem with Tucker format\n",
    "\n",
    "The main problem with the Tucker format is that it can not be used to represent **high-dimensional functions**\n",
    "\n",
    "We reduce $n^d$ complexity to $r^d$, which makes $d = 10,\\ldots, 20$ tracktable, but not more.\n",
    "\n",
    "Are there other ideas?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main motivation\n",
    "Matrices are good, so reduce everything to matrices! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting into halves\n",
    "Slit index set into two, get $n^{d/2} \\times n^{d/2}$ matrix, compute its SVD:\n",
    "\n",
    "$$A(I, J) = \\sum_{\\alpha=1}^r U_{I, \\alpha} V_{J, \\alpha},$$\n",
    "\n",
    "then we have $d_1 +1 $ and $d_2 + 1$ - dimensional tensors. Do it recursively!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another construction\n",
    "Another construction, so-called Hierarchical Tucker, forms the construction:\n",
    "\n",
    "1. Compute Tucker decomposition, get a $r \\times r \\times \\ldots \\times r$ array;\n",
    "2. Merge indices by two, and compute **transfer matrices** of size $r \\times r \\times r$.\n",
    "3. Go up the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest construction\n",
    "The simplest construction **separates** one index at a time and gives the **tensor-train** (or **matrix-product state**) decomposition\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx G_1(i_1) \\ldots G_d(i_d), $$\n",
    "\n",
    "where $G_k(i_k)$ is $r_{k-1} \\times r_k$ matrix for any fixed $r_k$.\n",
    "\n",
    "**Similar to Hidden Markov models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor-train\n",
    "Tensor-train (in physics known as matrix-product-state) representation has the form\n",
    "\n",
    "$$\n",
    "   A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d), \n",
    "$$\n",
    "where $G_k(i_k)$ has size $r_{k-1} \\times r_k$ and $r_0 = r_d = 1$. \n",
    "\n",
    "Now we have **d-1** ranks (we call them TT-ranks). The parametrization is defined by **TT-cores**.\n",
    "\n",
    "Why tensor train? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-ranks are matrix ranks\n",
    " Define unfoldings: \n",
    " \n",
    " $A_k = A(i_1 \\ldots i_k; i_{k+1} \\ldots i_d)$, $n^k \\times n^{d-k}$\n",
    " matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   Theorem: there exists a TT-decomposition with TT-ranks  \n",
    "   \n",
    "   $$r_k = \\mathrm{rank} A_k$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The proof \n",
    "Proof is simple and give the TT-SVD algorithm (in quantum information it is known as Vidal algorithm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability estimate\n",
    "No exact ranks in practice -- stability estimate! \n",
    "\n",
    "If $A_k = R_k + E_k$, $||E_k|| = \\varepsilon_k$\n",
    "  $$||\\mathbf{A}-\\mathbf{TT}||_F \\leq \\sqrt{\\sum_{k=1}^{d-1} \\varepsilon^2_k}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast and trivial linear algebra\n",
    " Addition, Hadamard product, scalar product, convolution \n",
    " \n",
    " All scale linear in $d$ \n",
    " \n",
    " $$C(i_1,\\ldots,i_d) = A(i_1,\\ldots,i_d) B(i_1,\\ldots,i_d)$$\n",
    "    \n",
    "   $$C_k(i_k) = A_k(i_k) \\otimes B_k(i_k),$$\n",
    "   ranks are multiplied\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rounding\n",
    "**Rounding** procedure solves the following problem: given a TT-representation\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) G_2(i_2) \\ldots G_d(i_d)$$\n",
    "\n",
    "find **another one** with smaller memory that approximates it with **threshold** $\\epsilon$.\n",
    "\n",
    "It can be done in $\\mathcal{O}(dnr^3)$ operations using the **linear structure** of the format:\n",
    "\n",
    "We can do orthogonalization of the cores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation in $d$-dimensions\n",
    "\n",
    "Oseledets, Tyrtyshnikov, TT-cross approximation of multidimensional arrays, which generalizes the matrix result.\n",
    "\n",
    "**Theorem:** A rank-$r$ tensor can be recovered from $\\mathcal{O}(dnr^2)$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantized tensor-train\n",
    "To used tensor decompositions, we have to **approximate** a continious problem by a discrete, where the **vector of unknowns** can be indexed by a $d$-index.\n",
    "\n",
    "1. (Standard) Given a function of $d$ variables, introduce a tensor-product basis set, and the tensor is the tensor of coefficients\n",
    "2. (Non-standard) Given a function of one variable, take it on a uniform grid with $2^d$ points, reshape into a $2 \\times \\ldots \\times 2$ $d$-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantized tensor train\n",
    "\n",
    "It is a mapping from 1D functions to $d$-dimensional arrays, using the following representation:\n",
    "\n",
    "$$v_k = f(x_k), \\quad k = 1, \\ldots, 2^d.$$\n",
    "\n",
    "$V(i_1, \\ldots, i_d) = v(a + i h)$, $\\quad i = i_1 + 2 i_2 + \\ldots 2^{d-1} i_d.$\n",
    "\n",
    "It gives $\\mathcal{O}(2dr^2) = \\mathcal{O}(\\log N)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of some basic functions\n",
    "Almost all important functions in 1D are of low QTT-ranks\n",
    "\n",
    "- $f(x) = \\exp(\\lambda x)$ -- rank $1$\n",
    "- $f(x) = \\sin( w x + \\alpha)$ -- rank $2$\n",
    "- $f(x)$ -- polynomial, rank $p+1$\n",
    "- Rational functions, even non-smooth functions (Weirstrass function has bounded QTT-ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wavelets and QTT\n",
    "\n",
    "For 2D functions, simple structures lead to small QTT-ranks (i.e. characteristic function of a triangle).\n",
    "<p>\n",
    "\n",
    "<div style=\"float: left; width: 40%; margin-right: 5%; margin-bottom: 0.5em;\">\n",
    "<p style=\"text-align:left\"> Characteristic function of the triangle, QTT-ranks=2 </p>\n",
    "<img src=\"triangle.jpg\" >\n",
    "</div>\n",
    "<div style=\"float: left; width: 40%; margin-right: 5%; margin-bottom: 0.5em;\">\n",
    "<p style=\"text-align:left\"> Characteristic function of the circle,  QTT-ranks full</p>\n",
    "<img src=\"circle.jpg\" >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is it so?\n",
    "\n",
    "The meaning of the QTT-rank is that you split the image into $2^k \\times 2^k$ patches, and then you look at the **linear dependence** between patches, and compute the basis. \n",
    "\n",
    "For the circle, due to grid artifacts, the dimension of the linear span is big.\n",
    "\n",
    "Alternative: interpret this as a filter bank, and transform locally.\n",
    "\n",
    "This leads to **WTT** transform, where the filters vary from level to level and are computed adaptively for the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problem:\n",
    "\n",
    "The optimization problem, which I do not know how to solve, is\n",
    "\n",
    "$y \\approx W s$, where $W$ is a special orthogonal matrix, and $s$ is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another application of low-rank structure\n",
    "In machine learning problems, often the low-rank structure comes not for good equality, but\n",
    "\n",
    "1. As regularizer, restricting the set of \"good\" solutions\n",
    "2. As initial approximation for a general optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example one: CNN optimizer\n",
    "\n",
    "http://arxiv.org/abs/1412.6553\n",
    "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition\n",
    "\n",
    "A convolutional neural network layer is just a contraction with a 4D tensor. We approximated this tensor by a canonical decomposition (using wonderful TensorLab by De Lathauwer et al.) \n",
    "\n",
    "and then put one more layer into the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example two: regularization\n",
    "\n",
    "Tensorizing neural networks, Novikov et. al.\n",
    "\n",
    "Instead of optimizing for fully-connected layer, this group from Skoltech optimized for a so-called **TT-matrix**.\n",
    "\n",
    "Result: no accuracy loss, 700x less number of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a multidimensional SVD\n",
    "\n",
    "A multidimensional SVD adds you a tool, an efficient structure you can work with.\n",
    "\n",
    "You can do:\n",
    "\n",
    "- Tensor completion\n",
    "- Solution of linear systems\n",
    "- Regularization of regression\n",
    "\n",
    "By reducing to the solution of low-rank constraint optimization: $$F(X) \\rightarrow \\min,$$\n",
    "\n",
    "subject to $X$ has \"low TT-rank\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riemannian optimization\n",
    "\n",
    "I became a big fan of Riemannian optimization techniques. \n",
    "\n",
    "The set of TT-tensors with bounded has is a **manifold**, thus we can write **projected gradient method**\n",
    "\n",
    "$$X_{k+1} = P_{\\mathcal{T}}(X_k + \\nabla F(X_k)),$$\n",
    "\n",
    "and then do **retraction** $R$ onto the manifold. Together with C. Lubich & B. Vandereycken we have designed a simplest\n",
    "\n",
    "retraction called **projector-splitting**, which is a half-step of ALS in disguise.\n",
    "\n",
    "$$X_{k+1} = I(X_k, \\nabla F(X_k)).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big mystery: how such methods converge\n",
    "The standard theory says that such methods convergence deteriorate when the curvature (i.e., inverse of the minimal singular value) is large. In practice: does not matter!\n",
    "\n",
    "$$x = Qx + f, \\quad \\Vert Q \\Vert \\leq \\delta, $$\n",
    "Solution $x^*$ has rank $5$ with $3$ large singular values, one $10^{-3}$ of the largest, one $10^{-6}$ of the largest.  \n",
    "<font color='blue'> Blue </font>: original gradient method, <font color='green'> Green </font>: Manifold-projected gradient, <font color='cyan'> Cyan </font>: The normal component of the error\n",
    "<img src=\"levels.png\" width=60%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Tensor train is a structure we can work with as efficient as with SVD\n",
    "- Riemannian optimization is great (nuclear norm? proxy? somebody, compare!)\n",
    "- The topology of low-rank matrices is not yet understood (it is linear!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "- oseledets.github.io\n",
    "- http://github.com/oseledets/TT-Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
