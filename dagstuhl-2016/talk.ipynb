{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'scroll': True,\n",
       " u'start_slideshow_at': 'selected',\n",
       " u'theme': 'sky',\n",
       " u'transition': 'zoom'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.paths import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "              'scroll': True,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensor trains and tensor networks\n",
    "##### Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CP-decomposition\n",
    "\n",
    "A Canonical Polyadic (CP) decomposition is the decomposition of a general tensor in the form\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) U_2(i_2, \\alpha) \\ldots U_d(i_d, \\alpha).$$\n",
    "\n",
    "Many work has been done (nice review by B. Bader & T. Kolda with all proper references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## My original motivation\n",
    "\n",
    "I have been working on low-rank approximation of matrices, where singular value decomposition (SVD) gives the best rank-$r$ approximation, which can be written as\n",
    "\n",
    "$$A_{ij} \\approx \\sum_{\\alpha=1}^r U_{i \\alpha} V_{j \\alpha}.$$\n",
    "\n",
    "And best rank-$r$ approximation always exists, efficient algorithms are in LAPACK, efficient sampling via skeleton decomposition is possible from $\\mathcal{O}(dnr)$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## My motivation\n",
    "\n",
    "Go to 3 indices:\n",
    "\n",
    "$$A_{ijk} \\approx \\sum_{\\alpha=1}^r U_{i \\alpha} V_{j \\alpha} W_{k \\alpha}.$$\n",
    "\n",
    "Everything is more complicated (there are exceptions, like if $r$ is equal to one of the mode sizes, you can do it via generalized eigenvalue/simultaneous matrix factorizations...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1:\n",
    "\n",
    "The first example of a tensor that is bad for CP is the following one:\n",
    "$$A(i_1, \\ldots, i_d) = i_1 + \\ldots + i_d.$$\n",
    "\n",
    "Define $$P(i_1, \\ldots, i_d, t) = (1 + i_1 t)(1 + i_2 t) \\ldots (1 + i_d t), $$\n",
    "then \n",
    "\n",
    "$$A = P'(0) = \\frac{P(h) - P(0)}{h} + \\mathcal{O}(h).$$\n",
    "\n",
    "**CP-rank** is **d**, can be approximated to any accuracy with **rank 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2:\n",
    "The CP-rank can depend on the field, the following example shows it:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = \\sin(h (i_1 +  \\ldots + i_d)).$$\n",
    "\n",
    "Simple trigonometry: $$r = 2^d$$\n",
    "\n",
    "Advanced trigonometry: $$r = d$$\n",
    "\n",
    "Complex arithmetics: $$r = 2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor of matrix-by-matrix multiplication\n",
    "\n",
    "There is an example of $9 \\times 9 \\times 9$ tensor, for which the value of the canonical rank is not known!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tucker decomposition\n",
    "\n",
    "Tucker decomposition is an old-time alternative to CPD, but it is amenable to the **curse of dimensionality**:\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha_1, \\ldots, \\alpha_d} G(\\alpha_1, \\ldots, \\alpha_d) U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha).$$\n",
    "\n",
    "The number of parameters is $dnr + r^d$ (very ok for compression purposes and 3D-..., but not for 100D).\n",
    "\n",
    "**Good news:** Quasioptimal approximation can be computed by SVD, best approximation always exist (closed set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inbetween\n",
    "\n",
    "\n",
    "A simple question: is there anything between CPD and Tucker that is free from inheritant curse of dimensionality, but is a **stable tensor format**?\n",
    "\n",
    "The answer is definitely yes, and the simplest example of such **new** tensor factorization is the **linear tensor network** or **tensor train** format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-format\n",
    "\n",
    "The Tensor is said to be in the TT-format if \n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d), $$\n",
    "\n",
    "where $G_k(i_k)$ for a fixed $i_k$ is a matrix of size $r_{k-1} \\times r_k$.\n",
    "\n",
    "Index form:\n",
    "$$A(i_1, \\ldots, i_d) = \\sum_{\\alpha_1, \\ldots, \\alpha_{d-1}} G_1(i_1, \\alpha_1) G_2(\\alpha_1,i_2, \\alpha_2) \\ldots G_d(\\alpha_d, i_d).$$\n",
    "\n",
    "<img src='tt.svg' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other stable manifolds\n",
    "\n",
    "The concept of **tensor networks** can be extended to other types of graphs, the **H-Tucker** approach being the most general; \n",
    "\n",
    "the TT-format is the simplest algebraically and typically there is no increase in complexity.\n",
    "\n",
    "In general, the tensor network **should not have loops**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loopy tensor networks are hard\n",
    "\n",
    "<img src='2d.svg' width=40%>\n",
    "\n",
    "Harder than CPD: even  computing a single element is exponential (the storage is not, every node of the network in a 5D tensor).\n",
    "\n",
    "(and in quantum information theory this serves as a justification for the **quantum computer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT as an intersection of matrix manifolds\n",
    "\n",
    "There exists a TT-decomposition with\n",
    "\n",
    "$$r_k = \\mathrm{rank}(A_k),$$\n",
    "\n",
    "where $A_k$ is the **k-th** unfolding of the tensor, i.e.\n",
    "\n",
    "$$A_{k} = A(i_1 \\ldots i_k; i_{k+1} \\ldots i_d).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-SVD\n",
    "\n",
    "The quasioptimal approximation can be computed via sequential SVD of the auxiliary matrices:\n",
    "\n",
    "$$A(i_1; i_2 \\ldots i_d) \\approx \\sum_{\\alpha=1}^{r_1} G_1(i_1, \\alpha_1) G_2(\\alpha_1 i_2; i_3 \\ldots i_d),$$\n",
    "\n",
    "then we separate $\\alpha_1 i_2$ by another SVD and so on.\n",
    "\n",
    "TT-SVD can be coded in $\\sim$ 50 lines of MATLAB / Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other important stuff\n",
    "\n",
    "\n",
    "- Always $r_k \\leq r$, where $r$ is the canonical rank.\n",
    "- Basic linear algebra in the TT-format\n",
    "- Fast rounding (i.e. TT with suboptimal ranks, costs $\\mathcal{O}(dnr^3)$ to compute the TT-SVD exactly)\n",
    "- **TT-cross approximation**\n",
    "- Open-source TT-Toolbox (both for MATLAB and Python)\n",
    "- Optimization over TT-manifolds\n",
    "- Dynamical approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-cross approximation\n",
    "\n",
    "Given the tensor $\\mathcal{A}$ as a black-box subroutine that allows us to compute any prescribed element.\n",
    "\n",
    "Suppose we have apriori knowledge that $r_k \\leq r$.\n",
    "\n",
    "Then the tensor can be exactly recovered from $\\mathcal{O}(dnr^2)$ samples, \n",
    "\n",
    "and we have a fairly robust **deterministic algorithm** (TT-cross) to compute those entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: the matrix case\n",
    "\n",
    "It is a generalization of the fabolous **skeleton decomposition** of a rank-$r$ matrix:\n",
    "\n",
    "any rank-$r$ matrix can be recovered from $r$ columns and $r$ rows as\n",
    "\n",
    "$$A = C \\widehat{A}^{-1} R.$$\n",
    "\n",
    "<img src='cross-pic.png' width=80%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol estimate\n",
    "Goreinov, Tyrtyshnikov:\n",
    "\n",
    "If $\\widehat{A}$ is of maximal volume (absolute value of the determinant), then\n",
    "\n",
    "$$\\Vert A - A_{skel} \\Vert_C \\leq (r + 1) \\sigma_{r+1}.$$\n",
    "\n",
    "The maximum volume (together with the **maxvol** algorithm) gives a simple algorithm  for the search of good/rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TT-cross & skeleton\n",
    "\n",
    "The TT-cross result is a direct generalization of the matrix skeleton decomposition (and also comes with several adaptive algorithms).\n",
    "\n",
    "Experience: The **randomization** is needed to improve robustness of the algorithms, but a good algorithm can not rely only on random sampling of entries, it should be adaptive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software packages\n",
    "\n",
    "- TT-Toolbox for MATLAB (since 2011), http://github.com/oseledets/TT-Toolbox\n",
    "- TT-Toolbox for Python (since 2013), http://github.com/oseledets/ttpy\n",
    "\n",
    "Implement basic operations, but also quite advanced algorithms like \n",
    "\n",
    "solving optimization problems with TT-rank constraint:\n",
    "\n",
    "$$F(X) \\rightarrow \\min$$ \n",
    "\n",
    "$X$ has low TT-ranks.\n",
    "\n",
    "This includes high-dimensional linear systems, eigenvalue problems, dynamical low-rank approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization over TT-manifolds\n",
    "\n",
    "An optimization over TT-manifolds is a separate story.\n",
    "\n",
    "The structure is polilynear, thus ALS is the most simple choice, but its convergence is **tremendously better**\n",
    "\n",
    "I.e. it is typical that the methods converges in $5-20$ iterations to very high accuracy.\n",
    "\n",
    "This is due to the **right parametrization** and stable representation (i.e. ALS in 2D converges very fast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application areas\n",
    "- TensorNet layer (Novikov et. al, NIPS 2015) - tremendous compression of a FC layer of a DNN\n",
    "- Inference in MRF (Novikov et. al, ICML 2014) - very accurate computations\n",
    "- Quantum chemistry\n",
    "- Multidimensional integrals\n",
    "- (multiscale) PDEs on very fine grids\n",
    "- Uncertainty quantification \n",
    "- $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hiearchical uncertainty quantification\n",
    "\n",
    "Given a model $$y = f(x)$$ where $x$ are now **random variables**, represent in a tensor-structured way the distribution of the output variables $$\\rho(y_1, \\ldots, y_k).$$\n",
    "\n",
    "That requires algebraic manipulations over multivariate functions, and this all can be implemented though the TT-formalism (including the non-linear operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantization idea\n",
    "\n",
    "The software is robust, so let us find **high dimensions** where there are no **high-dimensions**.\n",
    "\n",
    "Consider a 1D signal $$f(x) = \\sin x$$ sampled on a uniform grid with $2^d$ points.\n",
    "\n",
    "Reshape it into $2 \\times 2 \\times \\ldots \\times 2$ $d$-dimensional tensor.\n",
    "\n",
    "That gives you a TT-tensor of **rank 2**, thus giving $\\mathcal{O}(2dr^2) = \\mathcal{O}(\\log N)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PDEs with astronomically large number of \"virtual\" unknowns\n",
    "\n",
    "This gives a concept: take the simplest representation but with a huge **virtual** grid with $2^d$ points.\n",
    "\n",
    "Kazeev & Schwab (2015) have proved that for a wide class of 2D elliptic PDEs the solution has low-rank QTT structure.\n",
    "\n",
    "We have recently implemented a solver that allows to \"solve\" problems with $2^{60}$ unknowns on a laptop in seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where to start (literature)\n",
    "- [A literature survey of low‐rank tensor approximation techniques L. Grasedyck, D. Kressner, C. Tobler](http://onlinelibrary.wiley.com/doi/10.1002/gamm.201310004/pdf)\n",
    "- [Tensor decompositions and applications T.G. Kolda, B. W Bader](http://epubs.siam.org/doi/pdf/10.1137/07070111X) \n",
    "- Book in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Publications and software\n",
    "- http://oseledets.github.io -- Scientific Computing Group at Skoltech\n",
    "- http://github.com/oseledets/TT-Toolbox -- Tensor Trains in MATLAB\n",
    "- http://github.com/oseledets/ttpy -- Tensor Trains in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
