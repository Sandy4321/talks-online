{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'start_slideshow_at': 'selected', u'theme': 'sky', u'transition': 'zoom'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.utils.path import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Low-rank approximation: new application areas?\n",
    "#####Ivan Oseledets, Skolkovo Institute of Science and Technology \n",
    "##### oseledets.github.io, i.oseledets@skoltech.ru\n",
    "\n",
    "\n",
    "\n",
    "### Talk: http://goo.gl/mTewpH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skoltech\n",
    "- Founded in 2011 as a new Western-style university near Moscow http://faculty.skoltech.ru\n",
    "- In collaboration with MIT\n",
    "- No departments: priority areas \"IT, Bio, Space and Nuclear\"\n",
    "- At the moment, 160 master students, 30 professors, 40 postdocs, 50 PhD studens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MMMA-2015\n",
    "In August 23-28 2015 we hold the **4-th Moscow conference in Matrix Methods in Mathematics and Applications**.  \n",
    "\n",
    "Confirmed speakers: C. Schwab, B. Benner, J. White, D. Zorin, P.-A.Absil, A. Cichocki, P. Van Dooren.\n",
    "\n",
    "**Good time to visit Moscow** (i.e., due to the exchange rate drop from 35 roubles/dollar to 55 roubles/dollar). \n",
    "\n",
    "\n",
    "\n",
    "http://matrix.inm.ras.ru/mmma-2015/\n",
    "\n",
    "<img width=60% src=\"technopark-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Talk plan\n",
    "The main goal of this talk is to outline the difficulties in the usage of low-rank approximation in applications\n",
    "\n",
    "- Why non-quadratic can be important\n",
    "- Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low-rank approximation of matrices\n",
    "\n",
    "Given a matrix $A$ its **low-rank approximation** can be parametrized by the product $UV^{\\top}$.\n",
    "\n",
    "\n",
    "$$A \\approx UV^{\\top}, $$\n",
    "\n",
    "where $U$ is $n \\times r$ and $V$ is $m \\times r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing low-rank factorization\n",
    "\n",
    "Minimization of \n",
    "$$\\min_{\\mathrm{rank}(A_r)=r}\\Vert A - A_r \\Vert_2 = \\sigma_{r+1}$$\n",
    "\n",
    "can be done by the singular value decomposition (SVD).\n",
    "\n",
    "Or by **sampling** and **cross approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of low-rank approximation\n",
    "- Integral equations (off-diagonal blocks are low-rank)\n",
    "- Fast direct sparse solvers (see the poster by Daria Sushnikova)\n",
    "- Numerous machine learning/data mining tasks (Latent Semantic Analysis, collaborative filtering)\n",
    "\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "<font color='red'> Do we assume there is a good approximation? (i.e. $\\sigma_{r+1}$ is small?)  \n",
    "<p>\n",
    "</p>\n",
    " What to do, if $\\sigma_{r+1}$ is not small? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function approximation\n",
    "\n",
    "In the function approximation, low-rank approximation is equivalent to the **separation of variables**  \n",
    "\n",
    "\n",
    "$$f(x_1, \\ldots, x_d) \\approx \\sum_{\\alpha=1}^r U_1(x_1, \\alpha) \\ldots U_d(x_d, \\alpha),$$\n",
    "\n",
    "Often is better to use **stable** tensor formats (TT-format, HT-format).\n",
    "\n",
    "The main problem is to select **right variables** that **can be separated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Consider the **recommender system**: the **user-product** matrix $A$ contains only ones and zeros.\n",
    "\n",
    "In the low-rank approximation, do we need least squares approximation (when we care about bought/or not)?\n",
    "\n",
    "Of course, no (we optimize what we can optimize, not what we need to optimize).\n",
    "\n",
    "Now, the simplest example possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Identity matrix\n",
    "\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 1& 0 & 0 & 0 & 0 \\\\\n",
    "                           0 & 1 & 0 & 0 & 0 \\\\\n",
    "                           0 & 0  & 1& 0 & 0 \\\\\n",
    "                           0 & 0 & 0 & 1 & 0 \\\\\n",
    "                           0 & 0 & 0 & 0 & 1 \n",
    "                        \\end{bmatrix}\n",
    "                           $$\n",
    "                           \n",
    "                           What  is the rank of this matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Identity matrix(2)\n",
    "The rank is $2$. But if we care only about $0, 1$, we can use **the sign rank** which is defined as follows for a $0-1$ matrix $Y$:\n",
    "[Bouchard](http://statlearn.sfds.asso.fr/wp-content/uploads/2015/04/11-Bouchard.pdf)\n",
    "\n",
    "$$\\mathrm{rank}_{\\pm}(A) = \\min_{\\tau \\in \\mathbb{R}, X \\in \\mathbb{R}^{n \\times m}} \\{\\mathrm{rank}{X}; Y_{ij} = [X_{ij} + \\tau > 0] \\}$$\n",
    "\n",
    "What is the sign rank of the identity matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is equal to $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General principle\n",
    "\n",
    "It means, you have to use **absolutely different** functional (not quadratic, not matrix completion) in the minimization problem.\n",
    "\n",
    "For a sign-rank, a good surrogate is the **logistic loss**\n",
    "\n",
    "$$l(x, y) = \\log(1 + e^{-(2y - 1)x}) $$\n",
    "\n",
    "$$F(X, Y)  = \\sum_{ij} l(x_{ij}, y_{ij})$$\n",
    "\n",
    "Then we minimize $F(X, Y)$ given $Y$ subject to low-rank constraints on $X$.\n",
    "\n",
    "\n",
    "**Non-quadratic functional** -- forget about **alternating least squares**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riemannian optimization\n",
    "\n",
    "One of the most important properties of low-rank formats is the structure of the **tangent space**\n",
    "\n",
    "Given $A = U S V^{\\top}$, with orthonormal $U$ and $V$ the tangent space is defined as\n",
    "\n",
    "$$Z = U S V^{\\top} + \\delta U S V^{\\top} + U \\delta S V^{\\top} + U S \\delta V^{\\top},$$\n",
    "\n",
    "subject to $$\\delta U^{\\top} U + U^{\\top} \\delta U = \\delta V^{\\top} V + V^{\\top} \\delta V = 0$$\n",
    "\n",
    "The projector onto the tangent space is given by \n",
    "\n",
    "$$P_T(Z) = Z - (I - UU^{\\top}) Z (I - VV^{\\top})$$\n",
    "\n",
    "We can write down the projector for TT/HT formats as well!\n",
    "\n",
    "Given the projector, you can solve the problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization methods on manifolds\n",
    "\n",
    "For the low-rank manifold (matrix, TT, HT) we can efficiently compute the **projection** to the tangent space.\n",
    "\n",
    "\n",
    "The simplest is to **project the gradient onto the tangent space:**\n",
    "\n",
    "$$x_{k+1} = R(x_k + P_{\\mathcal{T}}(\\nabla F)), $$\n",
    "\n",
    "where $R$ is the mapping from the tangent space to the manifold, called **retraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example for sign-rank\n",
    "\n",
    "The gradient of \n",
    "\n",
    "$$F(X) = \\sum_{ij} L(X_{ij}) $$\n",
    "is given by\n",
    "\n",
    "$$\\frac{\\partial F(X)}{\\partial X_{kl}} = (d L/dt)(X_{kl}) = \\frac{2 Y_{kl} - 1}{1 + e^{(2Y_{kl} - 1) X_{kl}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 3\n",
    "Y = np.eye(n)\n",
    "Z = 2 * Y - 1\n",
    "def compute_grad(X):\n",
    "    return (-Z)/(1 + np.exp(-Z * X))\n",
    "\n",
    "def compute_func(X):\n",
    "    return np.sum(np.log(1 + np.exp(-Z * X)))\n",
    "\n",
    "def project_tangent(F, U, V):\n",
    "    return F - U.dot(U.T.dot(F)) - (F.dot(V)).dot(V.T)\n",
    "\n",
    "#Initialization\n",
    "r = 2\n",
    "u = np.random.randn(n, r)\n",
    "v = np.random.randn(n, r)\n",
    "u = np.linalg.qr(u, )[0]\n",
    "v = np.linalg.qr(v)[0]\n",
    "s = np.random.randn(r, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.573132258 0.0892293076862\n"
     ]
    }
   ],
   "source": [
    "#Run the code (no difficult step size selection)\n",
    "for _ in xrange(10000):\n",
    "    alpha = -0.05\n",
    "    X = u.dot(s).dot(v.T)\n",
    "    F = compute_grad(X)\n",
    "    F = project_tangent(F, u, v)\n",
    "    F0 =  compute_func(X)\n",
    "    X = X +  alpha * F     \n",
    "    u, s, v = np.linalg.svd(X)\n",
    "    u = u[:, :r]\n",
    "    s = np.diag(s[:r])\n",
    "    v = v[:r, :].T\n",
    "print compute_func(X ), np.linalg.norm(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  1.],\n",
       "       [ 1., -1.,  1.],\n",
       "       [ 1.,  1., -1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lyapunov equation\n",
    "\n",
    "Another example of non-quadratic functional comes from the **Lyapunov equation** [recent paper by Kolesnikov, Oseledets](http://arxiv.org/abs/1410.3335)\n",
    "$$AX + X A^{\\top} = y_0 y^{\\top}_0$$\n",
    "\n",
    "We look for the solution in the form \n",
    "\n",
    "$$X \\approx U Z U^{\\top}$$\n",
    "\n",
    "The functional (motivated by a seminal  paper by Y. Saad)\n",
    "\n",
    "$$F(U) = \\int_{0}^{\\infty} \\Vert y - \\widehat{y} \\Vert^2 dt,$$\n",
    "\n",
    "where $y = e^{At} y_0, \\widehat{y} = U e^{Bt} U^{\\top} y_0, B = U^{\\top} A U$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lyapunov equation (2)\n",
    "\n",
    "This functional can be computed\n",
    "\n",
    "$$ F(U) = \\mathrm{tr}(X) - 2 \\mathrm{tr}(U^{\\top}(P - UZ)) - \\mathrm{tr}(Z)$$\n",
    "\n",
    "where\n",
    "$$B Z + Z B^{\\top} = c_0 c^{\\top}_0, \\quad A P + P B^{\\top} = y_0 c^{\\top}_0, \\quad  c_0 = U^{\\top} y_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lyapunov equation (3)\n",
    "This leads to the Rational Krylov-type method with very simple shift generation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "- Use the functional you want to minimize, not quadratic loss\n",
    "- Try the Riemannian gradient descent first: simple to use, often converges well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Tensors \n",
    "\n",
    "- The most challenging problems are problems with **tensors** (curse of dimensionality, tricky optimization questions).\n",
    "\n",
    "- There are **tensor formats** that are **matrix low-rank approximation manifolds** in disguise\n",
    "\n",
    "- We can use efficient matrix techniques for working with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor-train  format\n",
    "\n",
    "The simplest SVD-based format is the **tensor-train format**\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "i.e. the **product of matrices, depending only on 1 index**, $G_k(i_k)$ is $r_{k-1} \\times r_k$ and $r_0 = r_d = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Canonical format\n",
    "\n",
    "A popular choice in function approximation is the **canonical** or sum-of-products format\n",
    "\n",
    "$$A(i_1, \\ldots, i_d) \\approx \\sum_{\\alpha=1}^r U_1(i_1, \\alpha) \\ldots U_d(i_d, \\alpha),$$\n",
    "\n",
    "i.e. sum of separable functions.\n",
    "\n",
    "Disadvantage: **it is not a stable format**: the best approximation may not exist, and may be hard to compute if we know that it exists!\n",
    "\n",
    "However, for a particular tensor $A$ **it can be very efficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor train\n",
    "The TT-format \n",
    "\n",
    "$$A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d),$$\n",
    "\n",
    "can be characterized by the following condition:\n",
    "\n",
    "$$\\mathrm{rank}(A_k) = r_k,$$\n",
    "\n",
    "where $A_k = A(i_1 i_2 \\ldots i_k; i_{k+1} \\ldots i_d)$ is the **k-th unfolding** of the tensor.\n",
    "\n",
    "I.e. it is the **intersection of low-rank matrix manifolds**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "We have a TT-Toolbox, both in MATLAB http://github.com/oseledets/TT-Toolbox and in Python http://github.com/oseledets/ttpy \n",
    "- Computing the TT representation (i.e. checking if there is such a representation)\n",
    "- Performing basic operations\n",
    "- Adaptive sampling algorithms (cross approximation)\n",
    "- Optimization algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor formats in machine learning.\n",
    "- Define an **undirected graph** $\\mathcal{G}$ with nodes corresponding to the variables (pixels of the image).\n",
    "- Define some positive functions $\\Psi_c(T_c; X, W)$ (called MRF **factors**) on the cliques of the graph $\\mathcal{G}$.\n",
    "- The model is then defined as follows:\n",
    "\t$$\n",
    "\tp(T | X, W) = \\frac{1}{Z(X, W)} \\prod\\limits_{c \\in \\mathcal{C}} \\Psi_c(T_c; X, W),\n",
    "\t$$\n",
    "\twhere $Z(X, W)$ is the normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation as a product of low TT-rank tensors\n",
    "\n",
    "Then, each factor depends only on the few variables, so its ranks are bounded.  \n",
    "\n",
    "Thus, the tensor is a product of low-rank tensors\n",
    "\n",
    "$$\n",
    "   A = A_1 \\circ A_2 \\ldots \\circ A_p,\n",
    "$$\n",
    "\n",
    "where $A_k(i_1, \\ldots, i_d) = A_{k1}(i_1) \\ldots A_{kd}(i_d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, we use the standard trick and obtain\n",
    "$$\\sum_{i_1, \\ldots, i_d} A(i_1, \\ldots, i_d) = \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{21}(i_1) \\otimes \\ldots \\otimes A_{p1}(i_1)\\Big)\\ldots \\Big(\\sum_{i_1} (A_{11}(i_1) \\otimes A_{2d}(i_d) \\otimes \\ldots \\otimes A_{pd}(i_d)\\Big),$$\n",
    "i.e. is now **a product of small-rank tensors**, so we **multiply them  and compress**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing marginals\n",
    "[Putting MRFs on a Tensor Train, A. Novikov, A. Rodomanov, A. Osokin, D. Vetrov, ICML-2014](https://www.dropbox.com/s/d479j6zocine232/Paper.pdf)\n",
    "<img width=60% src='spinglass.svg'>\n",
    "\t\t$$\\widehat{\\mathbf{P}}(\\vec{x}) = \\prod_{i=1}^n \\exp \\left ( -\\frac{1}{T} h_i x_i \\right ) \\prod_{(i, \\, j) \\in \\mathcal{E}} \\exp \\left (-\\frac{1}{T}c_{ij} x_i x_j \\right )$$\n",
    "<font size=6.8>\n",
    "Spin glass models, $T = 1$, $c_{ij} \\sim U[-f, f]$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization over TT-manifold\n",
    "\n",
    "Given $A(i_1, \\ldots, i_d) = G_1(i_1) \\ldots G_d(i_d)$ optimize over one core $G_k(i_k)$.  \n",
    "\n",
    "- Good for **quadratic functionals**, and also you can parametrize     \n",
    "    \n",
    "    $$\\mathrm{vec}(A) = Q \\mathrm{vec}(G_k),$$  \n",
    "    where $Q$ is orthonormal.\n",
    "- Bad for non-quadratic (frequent in machine learning!)\n",
    "\n",
    "Therefore, Riemanian optimization techniques are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Riemannian gradient descent\n",
    "\n",
    "$$x_{k+1} = R(x_k + P_{\\mathcal{T}}(\\nabla F)), $$\n",
    "\n",
    "The retraction step is easy, since the projection alwas has rank $2r$, so it can be done by **rounding**.\n",
    "\n",
    "The main problem is the computation of $$P_{\\mathcal{T}}(\\nabla F)$$ without exponential complexity.\n",
    "\n",
    "If it is possible, this is the way to go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: all-subsets regression\n",
    "\n",
    "Consider the **binary classification problem**. \n",
    "\n",
    "Log-regression is the simplest choice: given the **feature vector** $x$, \n",
    "\n",
    "we predict the probability to observe $y_i$\n",
    "\n",
    "$$p = Z \\exp(-y_i \\langle w, x_i \\rangle).$$\n",
    "\n",
    "I.e. the predictor variable is just the linear combination of the components of the feature vector,\n",
    "\n",
    "and $w$ is the **weight vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-subsets regression\n",
    "\n",
    "We can use other predictor variable, for example, select product of features (subsets) like\n",
    "\n",
    "$$w_1 x_1 + w_{125} x_1 x_2 x_5 + \\ldots $$\n",
    "\n",
    "We can code all **possible subsets** in this form by a vector of length $2^d$, or tensor of size $2 \\times \\ldots \\times 2$.\n",
    "\n",
    "(i.e. if there is $x_1$ in the term, or not). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The predictor variable is then **$t = \\langle W, X \\rangle$**, where $\\langle \\cdot \\rangle$ is the scalar product of tensors.\n",
    "\n",
    "$W$ is $2 \\times 2 \\times \\ldots \\times 2$ -- **weight tensor**\n",
    "\n",
    "We impose low-rank constraints on the $W$, to avoid **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problem\n",
    "The total loss is the sum of individual losses\n",
    "$$F(W) = \\sum_{k=1}^K f(y_i, \\langle X_i, W \\rangle),$$\n",
    "\n",
    "where $X_i$ is the low-rank tensor obtained from the **feature vector** $x_i$.\n",
    "\n",
    "The gradient is easily computatble:\n",
    "\n",
    "$$\\nabla F = \\sum_{k=1}^K \\frac{df}{dz}(y_i, \\langle X_i, W \\rangle) X_i,$$\n",
    "\n",
    "and **projection** onto the tangent space can be computed in a fast way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminary results\n",
    "On the problem [Otto](https://www.kaggle.com/c/otto-group-product-classification-challenge) from Kaggle, the larger the rank, the better is learning \n",
    "<img src='all-subsets.svg'>\n",
    "\n",
    "You have to train fast   \n",
    "(GPU implementation is necessary, as in the Deep Learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea for classifier\n",
    "\n",
    "We can go further and try to use the **low-rank regularization** as a constraint.\n",
    "\n",
    "**Deep Learning** is an extremely popular approach for classification/regression problems.\n",
    "\n",
    "Given a feature vector $x$, we construct a sequence of linear/non-linear layers\n",
    "\n",
    "$$x \\rightarrow C_1 \\rightarrow C_2 \\rightarrow \\ldots p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor Train as classifier\n",
    "\n",
    "The expression \n",
    "\n",
    "$$ x W_1(i_1) \\ldots W_d(i_d) $$\n",
    "\n",
    "($x$ is the feature vector) can be viewed as a sequence of \"linear\" layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification problem: setup\n",
    "\n",
    "We are given **feature vectors** $x$ (high-dimensional, $256 \\times 256$ images) and $y$ - classes that we want to predict.\n",
    "\n",
    "\n",
    "A log-regression uses a linear filter (matrix) $W$ and computes $p = Wx$, where $\\exp(p)$ are then  \n",
    "\n",
    "**the probabilities** to observe the corresponding class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-layer scheme\n",
    "\n",
    "A multilayer scheme uses a sequence of matrices $W(i_1, \\ldots, i_d)$ parametrized by a tensor-train\n",
    "\n",
    "$$W(i_1, \\ldots, i_d) = W_1(i_1) \\ldots W_d(i_d).$$\n",
    "\n",
    "It is equivalent to $n^d$ linear classifiers, but with constraints (otherwise we will have **overfitting**)\n",
    "\n",
    "This is a work in progress, but it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low-rank factorization as initialization\n",
    "\n",
    "You can use low-rank factorization to initialize other optimization methods.\n",
    "\n",
    "We have successfully speeded up the convolutional neural networks by factorizing a 4D tensor into the canonical format and then **fine-tuning it**.\n",
    "\n",
    "Thanks to the wonderful TensorLab toolbox by L. De Lathauwer!\n",
    "\n",
    "[Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition\n",
    "Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky,  ICLR 2015](http://arxiv.org/abs/1412.6553)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Publications and software\n",
    "- http://oseledets.github.io -- Scientific Computing Group at Skoltech\n",
    "- http://github.com/oseledets/TT-Toolbox -- Tensor Trains in MATLAB\n",
    "- http://github.com/oseledets/ttpy -- Tensor Trains in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
